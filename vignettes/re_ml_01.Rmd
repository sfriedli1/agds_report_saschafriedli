---
title: "Report Exercise 10"
output: 
  html_document:
    toc: true
author: Sascha Friedli
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,        
  warning = FALSE,    
  message = FALSE,    
  fig.align = "center"
)
```

Importing relevant libraries
```{R}
library(dplyr)
library(ggplot2)
library(readr)
library(tidyr)
library(caret)
library(recipes)

```

Download the data set of daily fluxes
```{R}
download.file(
  url = "https://raw.githubusercontent.com/geco-bern/agds_book/refs/heads/main/book/data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv",
  destfile = "../data/df_for_ml.csv"
)
```

Import the data set of daily fluxes and wrangle the data
```{R}
raw_data <- read.csv("../data/df_for_ml.csv") |>
    # select only the variables we are interested in
  dplyr::select(TIMESTAMP,
                GPP_NT_VUT_REF,    # the target
                ends_with("_QC"),  # quality control info
                ends_with("_F"),   # includes all all meteorological covariates
                -contains("JSB")   # weird useless variable
                ) |>

  # convert to a nice date object
  dplyr::mutate(TIMESTAMP = lubridate::ymd(TIMESTAMP)) |>

  # set all -9999 to NA
  mutate(across(where(is.numeric), ~na_if(., -9999)))
```

Import the data set and remove data of insufficient quality
```{R}
quality_data <- read.csv("../data/df_for_ml.csv") |>
  # select only the variables we are interested in
  dplyr::select(TIMESTAMP,
                GPP_NT_VUT_REF,    # the target
                ends_with("_QC"),  # quality control info
                ends_with("_F"),   # includes all all meteorological covariates
                -contains("JSB")   # weird useless variable
                ) |>

  # convert to a nice date object
  dplyr::mutate(TIMESTAMP = lubridate::ymd(TIMESTAMP)) |>

  # set all -9999 to NA
  mutate(across(where(is.numeric), ~na_if(., -9999))) |> 
  
  # retain only data based on >=80% good-quality measurements
  # overwrite bad data with NA (not dropping rows)
  dplyr::mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC < 0.8, NA, GPP_NT_VUT_REF),
                TA_F           = ifelse(TA_F_QC        < 0.8, NA, TA_F),
                SW_IN_F        = ifelse(SW_IN_F_QC     < 0.8, NA, SW_IN_F),
                LW_IN_F        = ifelse(LW_IN_F_QC     < 0.8, NA, LW_IN_F),
                VPD_F          = ifelse(VPD_F_QC       < 0.8, NA, VPD_F),
                PA_F           = ifelse(PA_F_QC        < 0.8, NA, PA_F),
                P_F            = ifelse(P_F_QC         < 0.8, NA, P_F),
                WS_F           = ifelse(WS_F_QC        < 0.8, NA, WS_F)) |> 

  # drop QC variables (no longer needed)
  dplyr::select(-ends_with("_QC"))

```

### Description of the data set
The data set that we use in this analysis contains daily measurements of the GPP and a range of meteorological variables. The data was recorded between 1997-2014 at the site in Davos, Switzerland.

## Comparison of the linear regression and KNN models

### Fitting and evaluating the linear regression model and the KNN
```{R}
# Data splitting
set.seed(1982)  # for reproducibility
split <- rsample::initial_split(quality_data, prop = 0.7, strata = "VPD_F")
daily_fluxes_train <- rsample::training(split)
daily_fluxes_test <- rsample::testing(split)

# Model and pre-processing formulation, use all variables but LW_IN_F
pp <- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, 
                      data = daily_fluxes_train |> drop_na()) |> 
  recipes::step_center(recipes::all_numeric(), -recipes::all_outcomes()) |>
  recipes::step_scale(recipes::all_numeric(), -recipes::all_outcomes())

# Fit linear regression model
mod_lm <- caret::train(
  pp, 
  data = daily_fluxes_train |> drop_na(), 
  method = "lm",
  trControl = caret::trainControl(method = "none"),
  metric = "RMSE"
)

# Fit KNN model
mod_knn <- caret::train(
  pp, 
  data = daily_fluxes_train |> drop_na(), 
  method = "knn",
  trControl = caret::trainControl(method = "none"),
  tuneGrid = data.frame(k = 8),
  metric = "RMSE"
)

```

Import function for visualizing the models:
```{R}
source("../R/eval_model.R")
```

Visualize Linear model:
```{R}
eval_model(mod = mod_lm, df_train = daily_fluxes_train, df_test = daily_fluxes_test)
```

Visualize KNN model:
```{R}
eval_model(mod = mod_knn, df_train = daily_fluxes_train, df_test = daily_fluxes_test)
```

### Interpret observed differences in the context of the bias-variance trade-off

Why is the difference between the evaluation on the training and the test set larger for the KNN model than for the linear regression model?
The difference between the evaluation on the training and the test set is larger for the KNN model, because the KNN model adjusts more closely to the data, because it can model more complex local structures in the data. Due to this it will achieve better performance on the training data. However, this high flexibilty to adjust to the training data makes it more prone to overfitting and when used to model test data, which is data that it has not been trained on, the performance decreases more significantly compared to the linear model.

Why does the evaluation on the test set indicate a better model performance of the KNN model than the linear regression model?
The KNN model shows better performance, because it can capture non-linear relationships between the target and predictor variables. A linear regression model can only model linear relationships in the data, where as the KNN model can also capture more complex local structures in the data. The relationship between the gross primary product (GPP) and the various meteorological variables likely is more complex than just linear and these more complex relations in the data can be modeled better with a KNN model than a linear regression model provided that k is chosen appropriately and overfitting is avoided. 

How would you position the KNN and the linear regression model along the spectrum of the bias-variance trade-off?
The linear regression model lies on the high-bias, low-variance end of the spectrum. It imposes the strong assumption of linearity and can there for not adapt to non-linear relationships in the data, potentially leading to underfitting.
The KNN model lies on the low-bias, high-variance end of the spectrum. Due to its high flexibility it can closely follow the training data. This decreases bias, but increases variance making it prone to overfitting. However, it is important to note that where on the spectrum of bias and variance a KNN model actually is, is strongly dependend on k. With an increasing k the model generally moves from a low-bias, high-variances to a more high-bias, low-variance end of the spectrum.

### Visualization of temporal variations of observed and modelled GPP for both models

Predict GPP with the two different models and attach TIMESTAMP to predictions.

Training data
```{r}
train_df <- daily_fluxes_train |> 
  drop_na() |> 
  select(TIMESTAMP) |>
  mutate(
  pred_lm = predict(mod_lm, newdata = daily_fluxes_train |> drop_na()),
  pred_knn = predict(mod_knn, newdata = daily_fluxes_train |> drop_na())
)
```

Test data
```{r}
pred_df <- daily_fluxes_test |> 
  drop_na() |> 
  select(TIMESTAMP) |> 
  mutate(
    pred_lm = predict(mod_lm, newdata = daily_fluxes_test |> drop_na()),
    pred_knn = predict(mod_knn, newdata = daily_fluxes_test |> drop_na())
  )
```


Plot raw data and add predicted data from the test set - lm model
```{r}
ggplot(data = raw_data, aes(x = TIMESTAMP, y = GPP_NT_VUT_REF, color = "Observed")) +
  geom_line(linewidth = 0.5) +
  geom_line(data = pred_df, aes(x = TIMESTAMP, y = pred_lm, color = "Predicted"), linewidth = 0.5) +
  labs(
    title = "Observed and Predicted GPP - Linear Regression Model",
    x = "Date",
    y = "GPP"
  ) +
  scale_color_manual(name = "Series", values = c("Observed" = "black", "Predicted" = "lightgreen")) +
  theme_classic()
```

Plot raw data and add predicted data from the test set - KNN model
```{r}
ggplot(data = raw_data, aes(x = TIMESTAMP, y = GPP_NT_VUT_REF, color = "Observed")) +
  geom_line(linewidth = 0.5) +
  geom_line(data = pred_df, aes(x = TIMESTAMP, y = pred_knn, color = "Predicted"), linewidth = 0.5) +
  labs(
    title = "Observed and Predicted GPP - KNN Model",
    x = "Date",
    y = "GPP"
  ) +
  scale_color_manual(name = "Series", values = c("Observed" = "black", "Predicted" = "lightgreen")) +
  theme_classic()
```

### Analysis of Observed vs. Predicted GPP:
The prediction with a linear regression model as well as with the KNN model follow the general structure of the observed values decently. However, when analyzing the predictions at the points in the data where we have extreme values in the observed data both models do not match those extremes. For the predictions that were produced using linear regression the outliers in the positive direction are underestimated by the model. On the other hand some of the outliers in the negative direction are overestimated by the predictions of the linear regression model. The predictions with the KNN model underestimated outliers in the predictions and the predictions with the KNN model were not as good as with the linear regression model. This difference could be down to the fact that the prediction with the KNN model is done based on the values of the k-nearest neighbours and there for the predictions are ankered towards the neighbour values that were not as extreme. The prediction with the linear regression model is not influenced as directly by the neighbour-values and can therefore extrapolate a little bit better.
Another interesting observation in the two plots is that our predictions only span over the years 2006-2014. For the predictions with the linear regression model and the KNN model we only used data with good quality. Our data with insufficient quality therefore is the data that is older. But we do not seem to have significant insufficients in the data after 2006, which is an indication, that the messurements nowdays are reliable.


## Exploring the role of k in a KNN model

R squared is used as an indicator for how "good" the model is. It indicates how much variation in GPP is explained by the model.

MAE is the mean absolute error and gives us the mean difference between observed and predicted values using absolute errors instead of the usual squared errors. By using absolute errors big errors have less importance than when looking at mean squared errors.

### Hypothesis:

As k approaches the total number of observations n, the KNN model becomes increasingly "blunt". It starts averaging over nearly the entire dataset for each prediction, effectively ignoring local patterns in the data. This leads to a model that is highly biased and underfitted, since it fails to capture the complexity of the underlying relationships. As a result, we expect to see a low R² (poor explanatory power) and a high MAE (poor prediction accuracy) on both the training and test sets. However, the model's variance will be low, since it behaves consistently across different subsets of data.

In contrast, as k approaches 1, the model becomes extremely flexible, predicting based on the nearest single data point. This leads to overfitting: the model can perfectly capture the training data (resulting in high R² and low MAE), but it fails to generalize to unseen data. On the test set, this causes a higher MAE and a lower R², due to the model’s high variance and sensitivity to noise.

### Testing the Hypothesis
Split data into test and train set
```{r}
set.seed(376)
split_knn <- rsample::initial_split(quality_data, prop = 0.7, strata = "VPD_F")
daily_fluxes_knn_train <- rsample::training(split)
daily_fluxes_knn_test <- rsample::testing(split)
```

Pre processing steps
```{r}
pp_knn <- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, 
                      data = daily_fluxes_knn_train |> drop_na()) |> 
  recipes::step_BoxCox(recipes::all_predictors()) |> 
  recipes::step_center(recipes::all_numeric(), -recipes::all_outcomes()) |>
  recipes::step_scale(recipes::all_numeric(), -recipes::all_outcomes())
```

Create empty metrics df
```{r}
results_both <- data.frame(type = character(), k = integer(), MAE = numeric())
```

Loop over different values of k and store model metrics
For better struture in the Markdown File I transferred this loop into a separate R-Script
```{R}
source("../R/run_KNN_loop.R")
```

Plot model metrics - MAE VS k
```{r}
ggplot(results_both, aes(x = k, y = MAE, color = type)) +
  geom_line(linewidth = 1, alpha = 0.7) +
  labs(
    title = "MAE VS k", 
    x = "k", 
    y = "MAE") +
  scale_color_manual(values = c("train" = "lightgreen", "test"= "lightblue")) +
  scale_y_continuous(limits = c(0.7,1.2)) +
  theme_minimal()
```

As predicted by our hypothesis, the plot shows that for values of k close to 1, the MAE on the training set is very low, while the MAE on the test set is considerably higher. This is a typical overfitting scenario: the model fits the training data almost perfectly (low bias), but fails to generalize well to new data due to high variance.
As k increases towards 100, the MAE on both the training and test sets gradually converge. For the test set the MAE is lower for k=100 compared to k=1, where as for the training set the MAE is significantly higher for k=100 compared to k=1. This indicates underfitting for k=100. The model becomes overly simplistic by averaging over many neighbors, leading to high bias and low variance. In this regime, the model fails to capture meaningful patterns in both training and test data, and thus both MAE values remain relatively high.
The transition between overfitting and underfitting is visible in the region where the test set MAE reaches its minimum. This point represents the optimal trade-off between bias and variance and corresponds to the best generalization performance. In this case, the optimal value appears to be around k=20.

### Find the best k
Extract MAE of the test set
```{r}
mae_test <- results_both |> 
  filter(type == "test")
```

To find the best value for k, we evaluate the MAE (Mean Absolute Error) on the test set. Since the MAE does not decrease continuously with increasing k, we avoid stopping the loop at the first sign of a worse result. Instead, we introduce a patience value. We set the patience level to 15, this way we do not quit the loop instantly the first time the MAE starts to increase. Because then we would maybe only capture a "local minima" and not the "global minima" due to fluctuations in the MAE.

Set loop features before start
```{r}
best_mae <- Inf
best_k <- NA
patience <- 15
counter <- 0
```

Create a result data frame
```{r}
results_test <- data.frame(k = integer(), MAE = numeric())
```

Loop over k for values 1:100 to find the best fitting k.
For better struture in the Markdown File I transferred this loop into a separate R-Script
```{R}
source("../R/early_stopping_KNN.R")

```

The best k for model generalization is therefore k = 19. The MAE is lowest for the test data at this k value and increases again with bigger k values.

Control with the previously extracted MAE values
```{r}
which.min(mae_test$MAE)
```

The evaluation of the loop shows that MAE is minimized with k = 19 for k in 1:100.

